const fs = require('fs')
const path = require('path')

const { GoogleGenerativeAI } = require('@google/generative-ai')
const { splitMessage } = require('../utils/splitmessages')
const { gemini_api_token } = require('../config/myconfig.json')
const { writeMemoryLog, getMemoryLog } = require('../utils/writelog')
const { createBasePromptForUser } = require('../services/basepersonal_users')
const { authorId } = require('../config/myconfig.json')
const { createBasePromptForAuthor } = require('../services/basepersonal_author')
const { getAllFilesFromCodeBase, getAllContentOutOfSrc } = require('../config/codebase')
const { getFlagCheck, setFlagCheck } = require('../events/flagcodebase')

// --------
const pathUser = path.join(__dirname, '../config/personaluser.json')
const pathBot = path.join(__dirname, '../config/personalbot.json')
const pathAuthor = path.join(__dirname, '../config/personalauthor.json')
// readFileSync lÃ  Ä‘á»c ná»™i dung trong file, cÃ²n readdirSync lÃ  liá»‡t kÃª file trong thÆ° má»¥c
const personalUser = JSON.parse(fs.readFileSync(pathUser), 'utf-8')
const personalBot = JSON.parse(fs.readFileSync(pathBot), 'utf-8')
const personalAuthor = JSON.parse(fs.readFileSync(pathAuthor), 'utf-8')

const genAI = new GoogleGenerativeAI(gemini_api_token)

async function modelAI(client, userId, textInput, chunkChannel) {
    const start = Date.now()
    let retries = 3
    while (retries--){
    const selectSystemInstruction = userId === authorId ? createBasePromptForAuthor(personalAuthor, personalBot) : createBasePromptForUser(personalUser, personalBot)

    let codeBaseContext = "KhÃ´ng cÃ³ thÃ´ng tin"
    let codeBaseContentOutOfSrc = "KhÃ´ng cÃ³ thÃ´ng tin"
    try {
    if (getFlagCheck() === true){
      if (userId === authorId){
        const files = await getAllFilesFromCodeBase()
        const filesOutOfSrc = await getAllContentOutOfSrc()
        codeBaseContext = files.map(f => `ðŸ“ [${f.folder}] ${f.file}\n${f.content}`).join('\n\n')
        codeBaseContentOutOfSrc = filesOutOfSrc.map(f => `ðŸ“„ ${f.file}\n${f.content}`).join('\n\n')
      }
    }
    const model = genAI.getGenerativeModel({
      model: "gemini-2.5-flash",
      systemInstruction : `${selectSystemInstruction}\n\n 
      VÃ  Ä‘Ã¢y lÃ  nhá»¯ng dÃ²ng code táº¡o ra chÃ­nh báº¡n: ${codeBaseContext}\n\n
      VÃ  Ä‘Ã¢y cÅ©ng lÃ  nhá»¯ng dÃ²ng code táº¡o ra chÃ­nh báº¡n, nhÆ°ng á»Ÿ bÃªn ngoÃ i thÆ° má»¥c source code: ${codeBaseContentOutOfSrc}\n\n
      âš ï¸ Náº¿u táº¥t cáº£ Ä‘á»u lÃ  "KhÃ´ng cÃ³ thÃ´ng tin", báº¡n khÃ´ng Ä‘Æ°á»£c Ä‘oÃ¡n. HÃ£y thÃ nh tháº­t ráº±ng báº¡n khÃ´ng cÃ³ quyá»n truy cáº­p codebase. NgÆ°á»i kiá»ƒm tra báº¡n cÃ³ thá»ƒ lÃ  chÃ­nh Author, lÃ  ngÆ°á»i cÃ³ UserID lÃ  607183227911667746.`
    })
    const memoryLog = getMemoryLog(userId, 20)
    const channel = await client.channels.fetch(chunkChannel)
          memoryLog.push({
            role: "user",
            parts: [{ text: textInput }]
          })
        const result = await model.generateContent({contents: memoryLog})
        const response = result.response
        const text = response.text()
        const elapsed = (Date.now() - start) / 1000
        const modelTextSliced = model.model.slice(7)

        if (text.length > 0){
          const chunkMessages = splitMessage(text)
          for (const chunk of chunkMessages){
            await channel.send(`${chunk}\n\n*> ${elapsed}s | ${modelTextSliced}*`)
          }
          writeMemoryLog({
            userId,
            prompt: textInput,
            response: text
          })
        }
        return;
      } catch (error){
      if (error.status == 503 && retries > 0) {
        console.warn("PhÃ­a Gemini Server bá»‹ quÃ¡ táº£i")   
        await new Promise(res => setTimeout(res, 2000))
        continue; 
      }
      if (error.status == 429){
        console.warn("TrÃ n output, nÃªn sáº½ cho háº§u gÃ¡i khÃ´ng nhá»› codebase")
        setFlagCheck(false)
        continue; // quay láº¡i tá»« Ä‘áº§u cháº¡y vá»›i viá»‡c false remember
      }
      console.error(error)
      await channel.send("AI bá»‹ ngu rá»“i, thÃ´ng cáº£m, hÃ£y thá»­ láº¡i xem")
      return;
    }
  }
}

module.exports = { modelAI }
